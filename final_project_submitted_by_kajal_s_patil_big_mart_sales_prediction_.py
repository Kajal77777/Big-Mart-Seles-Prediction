# -*- coding: utf-8 -*-
"""Final Project  Submitted by Kajal S Patil Big Mart Sales Prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_b-CQk3Z06eVCvnxDK0w6SPY-drV-ne
"""



"""#Final Project - Big Mart Sales Prediction

Date-30-sep-2021

Batch : ML-14

Submitted by :

1.Kajal Patil.

2.Dhanashri Gurav.

3.Priyanka Mhase.

4.Pooja Kadam.

The aim is to build a predictive model and find out the sales of each product at a particular store. Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sale
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

df_train = pd.read_csv(r'/content/drive/MyDrive/Train(1).csv')
df_test = pd.read_csv(r'/content/drive/MyDrive/Test.csv')

df_train.head()

df_train

df_train.isnull().sum()

df_train.shape

df_test.isnull().sum()

df_train.info()

df_train.describe()

df_train['Item_Weight'].describe()

df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean(),inplace=True)
df_test['Item_Weight'].fillna(df_test['Item_Weight'].mean(),inplace=True)

df_train.isnull().sum()

df_train['Item_Weight'].describe()

"""#### Outlet_size is categorical column so we fill it with Mode Imputation¶"""

df_train['Outlet_Size'].value_counts()

df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0],inplace=True)
df_test['Outlet_Size'].fillna(df_test['Outlet_Size'].mode()[0],inplace=True)

df_train

#df_test

"""### Selectin feature based on general requarment"""

df_train.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)
df_test.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)

#df_test
df_train

"""#EDA (Exploratery Data Analysys)
Exploratory data analysis (EDA) We have made our hypotheses and now we are ready to do some data exploration and come up with some inference. The goal for the EDA is to get some insight and if any irregularities are found we will correct that in the next section, Data Pre-Processing.
"""

!pip install klib

!pip install dtale

#EDA (Exploratery Data Analysys) by using dtale Lybrary
#EDA
import dtale

dtale.show(df_train)

dtale.show(df_train)

!pip install scikit-learn joblib

import sys

##Missing Values

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from statsmodels.stats import weightstats
import statistics as sts

plt.figure(figsize=(10,6))
sns.heatmap(df_train.corr(), annot=True)
plt.show()

"""#Using Klib Libraries
klib is a Python library for importing, cleaning, analyzing and preprocessing data. Explanations on key functionalities can be found on Medium / TowardsDataScience in the examples section or on YouTube (Data Professor).
"""

import klib

klib.cat_plot(df_train)

klib.corr_mat(df_train)# returns a color-encoded correlation matrix

klib.dist_plot(df_train) #returns a distribution plot

klib.missingval_plot(df_train)#missing value

"""# klib.clean - function for cleaning datasets
#Data Cleanin
"""

# klib.clean - function for cleaning datasets
#Data Cleanin

klib.data_cleaning(df_train)

klib.clean_column_names(df_train)

df_train.info()

df_train

"""## Lable Encoding"""

## Lable Encoding

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df_train=df_train.apply(le.fit_transform)

df_train

"""#One Hot Encoding"""

#One Hot Encoding

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder

df_train=df_train.apply(le.fit_transform)

df_train = pd.get_dummies(df_train, columns=['item_fat_content','outlet_size','outlet_location_type','outlet_type'])

df_train

"""#Train test split"""

X=df_train.drop('item_outlet_sales',axis=1)

Y=df_train['item_outlet_sales']

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, test_size=0.2,)

Y_train

X_train

"""#4) Standerization"""

#4) Standerization

### 4) Standerization
# standerization is for normlizing the data
X.describe()

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()

sc.fit(X)
X_train_std =sc.fit_transform(X_train)
X_test_std =sc.transform(X_test)

print('mean :',X_train.mean().round())
print('variance :',X_train.var().round())

X_train_std

Y_train

"""#Model Building
Model building in machine learning

A machine learning model is built by learning and generalizing from training data, then applying that acquired knowledge to new data it has never seen before to make predictions and fulfill its purpose. Lack of data will prevent you from building the model, and access to data isn't enough.

#Linear regression in Machine learning
Linear Regression is a machine learning algorithm based on supervised learning. ... Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output).
"""

#X_train

### Model Building
from sklearn.linear_model import  LinearRegression
lr = LinearRegression

lr= LinearRegression()

lr.fit(X_train_std, Y_train)

lr.fit(X_train_std, Y_train)

Y_pred_lr=lr.predict(X_test_std)

Y_test

from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error

print(r2_score(Y_test, Y_pred_lr))
print(mean_absolute_error(Y_test, Y_pred_lr))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_lr)))

"""#Random Forest
A random forest is a machine learning technique that's used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems. A random forest algorithm consists of many decision trees.
"""

### Random Forest

from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor()

rf.fit(X_train,Y_train)

Y_pred_rf = rf.predict(X_test)

print(r2_score(Y_test, Y_pred_rf))
print(mean_absolute_error(Y_test, Y_pred_rf))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_rf)))

"""#Logistic Regression
Logistic regression machine learning Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes. ... Mathematically, a logistic regression model predicts P(Y=1) as a function of X.
"""

### Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB

log = LogisticRegression()
log.fit(X_train,Y_train)

Y_pred_log = log.predict(X_test)

print(r2_score(Y_test, Y_pred_log))
print(mean_absolute_error(Y_test, Y_pred_log))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_log)))

cm_log=metrics.confusion_matrix(Y_test,Y_pred_log)
cm_log

def accuracy(confusion_matrix):
    diagonal_sum=confusion_matrix.trace()
    sum_of_all_elements=confusion_matrix.sum()
    return diagonal_sum/sum_of_all_elements

accuracy(cm_log)

"""#Ridge Regression
Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values to be far away from the actual values.
"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso

Ri = Ridge()

Ri.fit(X_train,Y_train)
Y_pred_Ri= Ri.predict(X_test)

print(r2_score(Y_test, Y_pred_Ri))
print(mean_absolute_error(Y_test, Y_pred_Ri))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_Ri)))

"""#Lasso
Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).
"""

ls=Lasso()

ls.fit(X_train,Y_train)
Y_pred_ls= ls.predict(X_test)

print(r2_score(Y_test, Y_pred_ls))
print(mean_absolute_error(Y_test, Y_pred_ls))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_ls)))

"""### SVM (Support Vector Machine)

SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs
"""

from sklearn import svm
from sklearn.svm import SVC

svm=SVC()

svm.fit(X_train,Y_train)
Y_pred_svm= svm.predict(X_test)

print(r2_score(Y_test, Y_pred_svm))
print(mean_absolute_error(Y_test, Y_pred_svm))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_svm)))

cm_svm=metrics.confusion_matrix(Y_test,Y_pred_svm)
cm_svm

def accuracy(confusion_matrix):
    diagonal_sum=confusion_matrix.trace()
    sum_of_all_elements=confusion_matrix.sum()
    return diagonal_sum/sum_of_all_elements

accuracy(cm_svm)

"""#Extra Tree Regressor
An extra-trees regressor. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.


"""

from sklearn.ensemble import ExtraTreesRegressor
es = ExtraTreesRegressor()

es.fit(X_train,Y_train)
Y_pred_es= es.predict(X_test)

print(r2_score(Y_test, Y_pred_es))
print(mean_absolute_error(Y_test, Y_pred_es))
print(np.sqrt(mean_squared_error(Y_test, Y_pred_es)))

"""#compare different algorithm with there Accueacy"""

#compare different algorithm with there Accueacy

print("Accuracy Through Linear Regression: ",0.6271315480729189)
print("Accuracy Through ExtraTreesRegressor: ",0.6013527541364836)
print("Accuracy Through RandomForestRegressor : ",0.6085176871279332)
print("Accuracy Through SVM : ",accuracy(cm_svm))

"""Accuracy Through Linear Regression:  0.6271315480729189
Accuracy Through ExtraTreesRegressor:  0.6013527541364836
Accuracy Through RandomForestRegressor :  0.6085176871279332
Accuracy Through SVM :  0.0017595307917888563

Basic model Linear Regression has best accuracy

Advanced Model Random Forest Regressor has best accuracy
"""

plt.subplots(figsize=(17,12))
sns.set(font_scale=1)
mask = np.triu(df_train.corr())
sns.heatmap(df_train.corr(),annot=True,vmin=-1, vmax=1, center= 0,mask=mask)

df_train.head()

print('Skewness: %f' % df_train['item_outlet_sales'].skew())
print('Kurtsis: %f' %df_train['item_outlet_sales'].kurt())

categorial_features = df_train.select_dtypes(include=[np.object])
categorial_features.head(2)

numerical_features = df_train.select_dtypes(include=[np.number])
numerical_features.head(2)

df_train.head()

#plt.figure(figsize=(15,5))
l = list(df_train['item_type'].unique())
chart = sns.countplot(df_train["item_type"])
chart.set_xticklabels(labels=l, rotation=90)

"""#Hypothesis Function
A hypothesis is a function that best describes the target in supervised machine learning. The hypothesis that an algorithm would come up depends upon the data and also depends upon the restrictions and bias that we have imposed on the data. ... Each individual possible way is known as the hypothesis.

1) SM Model

2) P-Values
"""

import pandas as pd
from sklearn import linear_model
import statsmodels.stats.api as sm

import statsmodels as sm

# statsmodel (linear regression)
import statsmodels.formula.api as smf
sm_model = smf.ols("item_weight ~ item_visibility	+ item_type	+ item_mrp	+ outlet_establishment_year	+ item_outlet_sales" , data= df_train).fit()

sm_model

#print coefficient
sm_model.params

sm_model.summary()

import statsmodels.formula.api as smf
sm_model2 = smf.ols("item_weight ~ item_visibility	+ item_type	+ item_mrp	+ outlet_establishment_year	+ item_outlet_sales" , data= df_train).fit()

sm_model2.summary()

#print the p-values for the model coefficients
sm_model.pvalues

for i in sm_model.pvalues:
    print(i)
    if i < 0.05:
        print("TRUE reject null")
    else:
        print("False accept null")

max(sm_model.pvalues)

min(sm_model.pvalues)

sns.countplot(df_train["item_outlet_sales"])

"""#Hyper Parameter Tuning"""

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# define models and parameters
model = RandomForestRegressor()
n_estimators = [10, 100, 1000]
max_depth=range(1,31)
min_samples_leaf=np.linspace(0.1, 1.0)
max_features=["auto", "sqrt", "log2"]
min_samples_split=np.linspace(0.1, 1.0, 10)

# define grid search
grid = dict(n_estimators=n_estimators)

#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=101)

grid_search_forest = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, 
                           scoring='r2',error_score=0,verbose=2,cv=2)

grid_search_forest.fit(X_train_std, Y_train)

# summarize results
print(f"Best: {grid_search_forest.best_score_:.3f} using {grid_search_forest.best_params_}")
means = grid_search_forest.cv_results_['mean_test_score']
stds = grid_search_forest.cv_results_['std_test_score']
params = grid_search_forest.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

grid_search_forest.best_params_

grid_search_forest.best_score_

Y_pred_rf_grid=grid_search_forest.predict(X_test_std)

r2_score(Y_test,Y_pred_rf_grid)

import sys

from statsmodels.stats import weightstats
import statistics as sts

plt.figure(figsize=(15,7))
sns.heatmap(df_train.corr(), annot=True)
plt.show()

klib.cat_plot(df_train)

klib.corr_plot(df_train, split='pos') # displaying only positive correlations, other settings include threshold, cmap...
klib.corr_plot(df_train, split='neg') # displaying only negative correlations

df_train.head()

klib.corr_plot(df_train, target='item_outlet_sales') # default representation of correlations with the feature column

klib.missingval_plot(df_train)

"""import pandas as pd.

​
import dtale.
import dtale. app as dtale_app.

​
dtale_app. USE_COLAB = True.

​
dtale. show(pd. DataFrame([1,2,3]))
"""

import pandas as pd
import dtale
import dtale.app as dtale_app
dtale_app. USE_COLAB = True
dtale. show(df_train)

"""just click on above link it will shows information about data"""

